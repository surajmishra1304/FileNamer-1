from __future__ import annotations
# Utility: SHA256 of text (for file naming)
def sha256_of_text(text: str) -> str:
    import hashlib
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

"""
unknown_part.py — One-file local app for detecting "Unknown Part" vs "Genuine" on iPhone images.

Features:
    Single-file app: training, evaluation, prediction, batch prediction, simple UI (Streamlit), and logging.
    Input: Image URL (or upload) → Output: label and confidence.
    Annotation Tab: paste many URLs, draw bounding boxes for "Unknown Part", save to CSV, and build a dataset.
    Dataset layout (no re-annotation required after first training):

        dataset/
            train/
                Genuine/
                Unknown Part/
            val/
                Genuine/
                Unknown Part/
            (optional) test/
                Genuine/
                Unknown Part/

    Saves: model weights (best and last), TorchScript export, class names JSON, training history JSON, metrics, and annotations CSV.
    Robust URL handling (timeouts, retries, content-type checks), EXIF orientation fix, SHA256 caching of downloaded images.
    Augmentations for robustness, early stopping, LR scheduler, mixed precision (auto), class-imbalance weighting.
    Deterministic seeds for reproducibility.

Run:
        pip install --upgrade pip
        pip install torch torchvision streamlit pillow requests matplotlib tqdm pyyaml streamlit-drawable-canvas
        streamlit run unknown_part.py

Author: ChatGPT (GPT-5 Thinking)
License: MIT
"""

















# ...existing code...


# --- Place main() definition at the very end, after all config/constants ---

def main():
    tcfg, appcfg = DEFAULT_TRAIN_CONFIG, DEFAULT_APP_CONFIG
    tcfg, appcfg = ui_sidebar(tcfg, appcfg)
    tabs = st.tabs(["Predict", "Batch", "Annotate", "Train", "Evaluate", "Metrics", "Logs"])
    with tabs[0]:
        ui_predict_single(appcfg, tcfg)
    with tabs[1]:
        ui_predict_batch(appcfg, tcfg)
    with tabs[2]:
        ui_annotate(appcfg, tcfg)
    with tabs[3]:
        ui_train(tcfg)
    with tabs[4]:
        ui_evaluate(tcfg)
    with tabs[5]:
        ui_metrics()
    with tabs[6]:
        ui_logs()

if __name__ == "__main__":
    main()

import logging
import math
import os
import json
import io
def set_torch_seed(seed: int = 42):
    """Set random seed for reproducibility."""
    try:
        torch.manual_seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    except Exception:
        pass

def save_runtime_config(tcfg, appcfg):
    """Stub for saving runtime config. Implement as needed."""
    pass
import random
import re
import shutil
import signal
import sys
import textwrap
import threading
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

# Third-party
import requests
from PIL import Image, ImageOps
import torch # type: ignore
import torch.nn as nn # type: ignore
import torch.optim as optim # type: ignore
from torch.optim.lr_scheduler import CosineAnnealingLR # type: ignore
from torch.utils.data import DataLoader # type: ignore
from torchvision import datasets, models, transforms # type: ignore
# UI
import streamlit as st # type: ignore
from streamlit_drawable_canvas import st_canvas # type: ignore
# Optional plotting
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# ========== Global Constants & Defaults ==========
APP_NAME = "iPhone Unknown Part Detector"
DEFAULT_MODEL_DIR = Path("artifacts")
DEFAULT_MODEL_DIR.mkdir(parents=True, exist_ok=True)
MODEL_PATH_BEST = DEFAULT_MODEL_DIR / "model_best.pth"
MODEL_PATH_LAST = DEFAULT_MODEL_DIR / "model_last.pth"
MODEL_TORCHSCRIPT = DEFAULT_MODEL_DIR / "model_scripted.pt"
LABELS_PATH = DEFAULT_MODEL_DIR / "class_names.json"
HISTORY_PATH = DEFAULT_MODEL_DIR / "training_history.json"
METRICS_PATH = DEFAULT_MODEL_DIR / "metrics.json"
RUNTIME_CONFIG_PATH = DEFAULT_MODEL_DIR / "runtime_config.json"
IMAGE_CACHE_DIR = DEFAULT_MODEL_DIR / "image_cache"
ANNOTATIONS_CSV = DEFAULT_MODEL_DIR / "annotations.csv"
LOG_DIR = DEFAULT_MODEL_DIR / "logs"
LOG_FILE = LOG_DIR / "app.log"
IMAGE_CACHE_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)
DEFAULT_DATASET_DIR = Path("dataset")

# ========== Logging Setup ==========
LOGGER = logging.getLogger(APP_NAME)
LOGGER.setLevel(logging.DEBUG)
class _StreamlitHandler(logging.Handler):
    def __init__(self, capacity: int = 1000):
        super().__init__()
        self.capacity = capacity
        self.buffer: List[str] = []
    def emit(self, record: logging.LogRecord) -> None:
        msg = self.format(record)
        self.buffer.append(msg)
        if len(self.buffer) > self.capacity:
            self.buffer.pop(0)
STREAMLIT_HANDLER = _StreamlitHandler(capacity=2000)
STREAMLIT_HANDLER.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s", "%Y-%m-%d %H:%M:%S"))
LOGGER.addHandler(STREAMLIT_HANDLER)
from logging.handlers import RotatingFileHandler
FILE_HANDLER = RotatingFileHandler(LOG_FILE, maxBytes=2_000_000, backupCount=5, encoding="utf-8")
FILE_HANDLER.setLevel(logging.DEBUG)
FILE_HANDLER.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(name)s | %(message)s", "%Y-%m-%d %H:%M:%S"))
LOGGER.addHandler(FILE_HANDLER)
CONSOLE_HANDLER = logging.StreamHandler(sys.stdout)
CONSOLE_HANDLER.setLevel(logging.INFO)
CONSOLE_HANDLER.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
LOGGER.addHandler(CONSOLE_HANDLER)

# ========== Dataclasses & Helper Classes ==========
SEED = 42
random.seed(SEED)

# ========== Dataclasses & Helper Classes ==========
from dataclasses import dataclass, field

@dataclass
class TrainConfig:
    data_dir: str = str(DEFAULT_DATASET_DIR)
    img_size: int = 224
    batch_size: int = 16
    epochs: int = 10
    lr: float = 1e-3
    weight_decay: float = 1e-4
    early_stop_patience: int = 5
    model_name: str = "resnet50"
    freeze_backbone: bool = False
    mixup_alpha: float = 0.0
    cutmix_alpha: float = 0.0
    amp: bool = True
    num_workers: int = 0

@dataclass
class AppConfig:
    request_timeout: int = 15
    max_image_size_mb: int = 10
    allowed_content_types: tuple = ("image/jpeg", "image/png", "image/webp")
    retry_attempts: int = 3
    retry_backoff_sec: float = 1.5

DEFAULT_TRAIN_CONFIG = TrainConfig()
DEFAULT_APP_CONFIG = AppConfig()

class ImageFetcher:
    def __init__(self, app_cfg: AppConfig):
        self.app_cfg = app_cfg
    def fetch_bytes(self, url: str) -> bytes:
        last_exc = None
        for attempt in range(self.app_cfg.retry_attempts):
            try:
                resp = requests.get(url, timeout=self.app_cfg.request_timeout)
                resp.raise_for_status()
                ctype = resp.headers.get("content-type", "")
                if not any(ctype.startswith(t) for t in self.app_cfg.allowed_content_types):
                    raise ValueError(f"Content-Type {ctype} not allowed")
                if int(resp.headers.get("content-length", 0)) > self.app_cfg.max_image_size_mb * 1024 * 1024:
                    raise ValueError("Image too large")
                return resp.content
            except Exception as e:
                last_exc = e
                time.sleep(self.app_cfg.retry_backoff_sec * (attempt + 1))
        raise RuntimeError(f"Failed to fetch image after retries: {last_exc}")
    def fetch(self, url: str) -> Image.Image:
        data = self.fetch_bytes(url)
        try:
            img = Image.open(io.BytesIO(data))
            img = ImageOps.exif_transpose(img).convert("RGB")
        except Exception as e:
            LOGGER.exception("Failed to parse image: %s", e)
            raise
        return img
# =============================
# Dataset Helpers
# =============================
@dataclass
class DatasetSummary:
    class_counts: Dict[str, int]
    total_images: int
    num_classes: int

def summarize_imagefolder(root: str) -> DatasetSummary:
    root_path = Path(root)
    if not root_path.exists():
        raise FileNotFoundError(f"Dataset path does not exist: {root}")

    class_counts: Dict[str, int] = {}
    for phase in ["train", "val", "test"]:
        phase_dir = root_path / phase
        if not phase_dir.exists():
            continue
        ds = datasets.ImageFolder(phase_dir)
        counts = {ds.classes[i]: 0 for i in range(len(ds.classes))}
        for _, label in ds.samples:
            class_name = ds.classes[label]
            counts[class_name] += 1
        LOGGER.info("Phase '%s' counts: %s", phase, counts)
        for k, v in counts.items():
            class_counts[k] = class_counts.get(k, 0) + v

    total = sum(class_counts.values())
    return DatasetSummary(class_counts=class_counts, total_images=total, num_classes=len(class_counts))

# =============================
# Model Factory
# =============================
class ModelFactory:
    @staticmethod
    def create_backbone(name: str, num_classes: int, freeze_backbone: bool) -> nn.Module:
        name = name.lower()
        if name == "resnet18":
            model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
            in_features = model.fc.in_features
            if freeze_backbone:
                for p in model.parameters():
                    p.requires_grad = False
            model.fc = nn.Linear(in_features, num_classes)
            return model
        elif name == "resnet50":
            model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
            in_features = model.fc.in_features
            if freeze_backbone:
                for p in model.parameters():
                    p.requires_grad = False
            model.fc = nn.Linear(in_features, num_classes)
            return model
        elif name == "efficientnet_v2_s":
            try:
                model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)
                in_features = model.classifier[-1].in_features
                if freeze_backbone:
                    for p in model.parameters():
                        p.requires_grad = False
                model.classifier[-1] = nn.Linear(in_features, num_classes)
                return model
            except Exception as e:
                LOGGER.warning("EfficientNet V2 not available, fallback to resnet50: %s", e)
                return ModelFactory.create_backbone("resnet50", num_classes, freeze_backbone)
        else:
            LOGGER.warning("Unknown model '%s', fallback to resnet50", name)
            return ModelFactory.create_backbone("resnet50", num_classes, freeze_backbone)

# =============================
# Augmentations
# =============================
def build_transforms(img_size: int, is_train: bool) -> transforms.Compose:
    if is_train:
        return transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomApply([transforms.ColorJitter(0.2, 0.2, 0.2, 0.1)], p=0.7),
            transforms.RandomAffine(degrees=12, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=4),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
    else:
        return transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

# =============================
# Mixup / CutMix (optional)
# =============================
def rand_bbox(size, lam):
    W = size[2]
    H = size[3]
    cut_rat = math.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    cx = random.randint(0, W)
    cy = random.randint(0, H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2

try:
    import numpy as np
except Exception:
    class _NP:
        @staticmethod
        def clip(x, a, b):
            return max(a, min(b, x))
    np = _NP()  # type: ignore

# =============================
# Training & Evaluation
# =============================
class Trainer:
    def __init__(self, cfg: TrainConfig, data_dir: str):
        self.cfg = cfg
        self.data_dir = data_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        LOGGER.info("Using device: %s", self.device)

        # Datasets
        train_dir = Path(data_dir) / "train"
        val_dir = Path(data_dir) / "val"
        if not train_dir.exists() or not val_dir.exists():
            raise FileNotFoundError("Expected 'train' and 'val' folders inside data_dir")

        self.train_dataset = datasets.ImageFolder(train_dir, transform=build_transforms(cfg.img_size, True))
        self.val_dataset = datasets.ImageFolder(val_dir, transform=build_transforms(cfg.img_size, False))
        self.class_names = self.train_dataset.classes
        LOGGER.info("Classes: %s", self.class_names)

        # Save class names
        LABELS_PATH.write_text(json.dumps(self.class_names, indent=2))
        LOGGER.info("Saved class names -> %s", LABELS_PATH)

        # Imbalance weights
        counts = [0] * len(self.class_names)
        for _, y in self.train_dataset.samples:
            counts[y] += 1
        LOGGER.info("Train class counts: %s", counts)
        total = sum(counts) if sum(counts) else 1
        class_weights = [total / max(c, 1) for c in counts]
        LOGGER.info("Class weights: %s", [f"{w:.3f}" for w in class_weights])

        self.train_loader = DataLoader(self.train_dataset, batch_size=cfg.batch_size, shuffle=True,
                                       num_workers=cfg.num_workers, pin_memory=True)
        self.val_loader = DataLoader(self.val_dataset, batch_size=cfg.batch_size, shuffle=False,
                                     num_workers=cfg.num_workers, pin_memory=True)

        # Model
        self.model = ModelFactory.create_backbone(cfg.model_name, len(self.class_names), cfg.freeze_backbone)
        self.model.to(self.device)

        # Loss & Optim
        self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=self.device))
        self.optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=cfg.lr, weight_decay=cfg.weight_decay)
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=max(cfg.epochs - 1, 1))

        self.scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)

        self.best_val_acc = 0.0
        self.epochs_no_improve = 0
        self.history: Dict[str, List[float]] = {"train_loss": [], "val_loss": [], "val_acc": []}

    def _apply_mix_methods(self, inputs, targets):
        if self.cfg.mixup_alpha > 0.0 and self.cfg.cutmix_alpha > 0.0:
            lam = np.random.beta(self.cfg.cutmix_alpha, self.cfg.cutmix_alpha)
            rand_index = torch.randperm(inputs.size()[0]).to(self.device)
            target_a = targets
            target_b = targets[rand_index]
            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)
            inputs[:, :, bby1:bby2, bbx1:bbx2] = inputs[rand_index, :, bby1:bby2, bbx1:bbx2]
            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))
            return inputs, target_a, target_b, lam, "cutmix"
        elif self.cfg.mixup_alpha > 0.0:
            lam = np.random.beta(self.cfg.mixup_alpha, self.cfg.mixup_alpha)
            rand_index = torch.randperm(inputs.size()[0]).to(self.device)
            mixed_x = lam * inputs + (1 - lam) * inputs[rand_index, :]
            target_a, target_b = targets, targets[rand_index]
            return mixed_x, target_a, target_b, lam, "mixup"
        else:
            return inputs, targets, None, None, None

    def _mix_loss(self, outputs, ta, tb, lam):
        return lam * self.criterion(outputs, ta) + (1 - lam) * self.criterion(outputs, tb)

    def train(self):
        LOGGER.info("Starting training for %d epochs", self.cfg.epochs)
        for epoch in range(1, self.cfg.epochs + 1):
            t0 = time.time()
            train_loss = self._train_one_epoch(epoch)
            val_loss, val_acc = self._validate(epoch)
            self.scheduler.step()

            self.history["train_loss"].append(train_loss)
            self.history["val_loss"].append(val_loss)
            self.history["val_acc"].append(val_acc)

            # Save last
            torch.save(self.model.state_dict(), MODEL_PATH_LAST)
            LOGGER.info("Saved last checkpoint -> %s", MODEL_PATH_LAST)

            improved = val_acc > self.best_val_acc
            if improved:
                self.best_val_acc = val_acc
                self.epochs_no_improve = 0
                torch.save(self.model.state_dict(), MODEL_PATH_BEST)
                LOGGER.info("✨ New best model (acc=%.3f). Saved -> %s", val_acc, MODEL_PATH_BEST)
                try:
                    self._export_torchscript()
                except Exception as e:
                    LOGGER.warning("TorchScript export failed: %s", e)
            else:
                self.epochs_no_improve += 1
                LOGGER.info("No improvement for %d epoch(s)", self.epochs_no_improve)
                if self.epochs_no_improve >= self.cfg.early_stop_patience:
                    LOGGER.info("Early stopping triggered")
                    break

            HISTORY_PATH.write_text(json.dumps(self.history, indent=2))
            LOGGER.info("Updated training history -> %s", HISTORY_PATH)
            LOGGER.info("Epoch %d done in %.1fs | train_loss=%.4f val_loss=%.4f val_acc=%.3f",
                        epoch, time.time() - t0, train_loss, val_loss, val_acc)

        METRICS_PATH.write_text(json.dumps({"best_val_acc": self.best_val_acc}, indent=2))
        LOGGER.info("Final best_val_acc=%.3f -> %s", self.best_val_acc, METRICS_PATH)

    def _train_one_epoch(self, epoch: int) -> float:
        self.model.train()
        running_loss = 0.0
        num_batches = len(self.train_loader)
        st_progress = None
        if st.session_state.get("ui_mode", False):
            st_progress = st.progress(0, text=f"Training epoch {epoch}")

        for i, (inputs, targets) in enumerate(self.train_loader):
            inputs = inputs.to(self.device, non_blocking=True)
            targets = targets.to(self.device, non_blocking=True)

            self.optimizer.zero_grad(set_to_none=True)

            mixed = self._apply_mix_methods(inputs, targets)
            if len(mixed) == 5 and mixed[-1] in ("mixup", "cutmix"):
                inputs, ta, tb, lam, _ = mixed
                with torch.cuda.amp.autocast(enabled=self.cfg.amp):
                    outputs = self.model(inputs)
                    loss = self._mix_loss(outputs, ta, tb, lam)
            else:
                with torch.cuda.amp.autocast(enabled=self.cfg.amp):
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, targets)

            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()

            running_loss += loss.item()

            if st_progress is not None:
                st_progress.progress((i + 1) / num_batches, text=f"Epoch {epoch} – Batch {i+1}/{num_batches}")

            if (i + 1) % 10 == 0 or (i + 1) == num_batches:
                LOGGER.debug("Epoch %d | Batch %d/%d | Loss %.4f", epoch, i + 1, num_batches, loss.item())

        return running_loss / max(1, num_batches)

    def _validate(self, epoch: int) -> Tuple[float, float]:
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        num_batches = len(self.val_loader)
        with torch.no_grad():
            for i, (inputs, targets) in enumerate(self.val_loader):
                inputs = inputs.to(self.device, non_blocking=True)
                targets = targets.to(self.device, non_blocking=True)
                with torch.cuda.amp.autocast(enabled=self.cfg.amp):
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, targets)
                running_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                correct += (preds == targets).sum().item()
                total += targets.size(0)
        val_loss = running_loss / max(1, num_batches)
        val_acc = correct / max(1, total)
        LOGGER.info("Validation: loss=%.4f acc=%.3f (epoch %d)", val_loss, val_acc, epoch)
        return val_loss, val_acc

    def _export_torchscript(self):
        self.model.eval()
        dummy = torch.randn(1, 3, self.cfg.img_size, self.cfg.img_size, device=self.device)
        traced = torch.jit.trace(self.model, dummy)
        torch.jit.save(traced, MODEL_TORCHSCRIPT)
        LOGGER.info("Exported TorchScript -> %s", MODEL_TORCHSCRIPT)

# =============================
# Inference
# =============================
class InferenceEngine:
    def __init__(self, img_size: int = 224, model_name: str = "resnet50"):
        self.img_size = img_size
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.class_names = self._load_class_names()
        self.model = ModelFactory.create_backbone(model_name, len(self.class_names))
        self.model.to(self.device)
        self._load_weights()
        self.model.eval()
        self.preprocess = build_transforms(img_size, is_train=False)

    def _load_class_names(self) -> List[str]:
        if not LABELS_PATH.exists():
            raise FileNotFoundError(f"Missing class names file: {LABELS_PATH}. Train once before prediction.")
        return json.loads(LABELS_PATH.read_text())

    def _load_weights(self):
        path = MODEL_PATH_BEST if MODEL_PATH_BEST.exists() else MODEL_PATH_LAST
        if not path.exists():
            raise FileNotFoundError("No trained weights found. Train the model first.")
        state = torch.load(path, map_location=self.device)
        self.model.load_state_dict(state)
        LOGGER.info("Loaded weights from %s", path)

    @torch.inference_mode()
    def predict_pil(self, img: Image.Image) -> Tuple[str, float]:
        tensor = self.preprocess(img).unsqueeze(0).to(self.device)
        outputs = self.model(tensor)
        probs = torch.softmax(outputs, dim=1).squeeze(0)
        conf, idx = torch.max(probs, dim=0)
        return self.class_names[int(idx)], float(conf.item())

# =============================
# Visualization Utils
# =============================
def plot_confusion_matrix(cm: List[List[int]], class_names: List[str]) -> bytes:
    fig = plt.figure(figsize=(4.5, 4))
    ax = plt.gca()
    ax.imshow(cm)
    ax.set_xlabel("Predicted")
    ax.set_ylabel("True")
    ax.set_xticks(range(len(class_names)))
    ax.set_yticks(range(len(class_names)))
    ax.set_xticklabels(class_names, rotation=45, ha="right")
    ax.set_yticklabels(class_names)
    for i in range(len(class_names)):
        for j in range(len(class_names)):
            ax.text(j, i, str(cm[i][j]), va='center', ha='center')
    fig.tight_layout()
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    plt.close(fig)
    return buf.getvalue()

def render_training_curves(history: Dict[str, List[float]]) -> bytes:
    fig = plt.figure(figsize=(6, 4))
    ax1 = plt.gca()
    ax1.plot(history.get("train_loss", []), label="Train Loss")
    ax1.plot(history.get("val_loss", []), label="Val Loss")
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("Loss")
    ax1.legend()
    fig.tight_layout()
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    plt.close(fig)

    fig2 = plt.figure(figsize=(6, 4))
    ax2 = plt.gca()
    ax2.plot(history.get("val_acc", []), label="Val Acc")
    ax2.set_xlabel("Epoch")
    ax2.set_ylabel("Accuracy")
    ax2.legend()
    fig2.tight_layout()
    buf2 = io.BytesIO()
    fig2.savefig(buf2, format='png', bbox_inches='tight')
    plt.close(fig2)

    img1 = Image.open(io.BytesIO(buf.getvalue()))
    img2 = Image.open(io.BytesIO(buf2.getvalue()))
    w = max(img1.width, img2.width)
    combined = Image.new('RGB', (w, img1.height + img2.height), (255, 255, 255))
    combined.paste(img1, (0, 0))
    combined.paste(img2, (0, img1.height))
    out = io.BytesIO()
    combined.save(out, format='png')
    return out.getvalue()

# =============================
# Batch Prediction (CSV URLs)
# =============================
def parse_csv_urls(text: str) -> List[str]:
    urls: List[str] = []
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        parts = re.split(r",|\s+", line)
        for p in parts:
            p = p.strip()
            if p.startswith("http://") or p.startswith("https://"):
                urls.append(p)
    return urls

# =============================
# NEW: Annotation helpers
# =============================
def _ensure_dirs(root: Path):
    for split in ["train", "val"]:
        for cls in ["Genuine", "Unknown Part"]:
            (root / split / cls).mkdir(parents=True, exist_ok=True)

def _save_annotations_csv(rows: List[Dict[str, Any]]):
    # rows: [{"url":..., "label":..., "boxes":..., "img_w":..., "img_h":...}, ...]
    # boxes = list of dicts: {"x":..., "y":..., "w":..., "h":...} in pixel coordinates of original image
    lines = ["url,label,img_w,img_h,boxes_json"]
    for r in rows:
        lines.append(f'{r["url"]},{r["label"]},{r["img_w"]},{r["img_h"]},{json.dumps(r["boxes"])}')
    ANNOTATIONS_CSV.write_text("\n".join(lines), encoding="utf-8")
    LOGGER.info("Saved annotations -> %s", ANNOTATIONS_CSV)

def _load_annotations_csv() -> List[Dict[str, Any]]:
    if not ANNOTATIONS_CSV.exists():
        return []
    rows: List[Dict[str, Any]] = []
    for i, line in enumerate(ANNOTATIONS_CSV.read_text(encoding="utf-8").splitlines()):
        if i == 0:
            continue
        if not line.strip():
            continue
        # naive CSV split (no commas inside JSON due to json.dumps)
        parts = line.split(",", 4)
        if len(parts) < 5:
            continue
        url, label, img_w, img_h, boxes_json = parts
        try:
            boxes = json.loads(boxes_json)
        except Exception:
            boxes = []
        rows.append({"url": url, "label": label, "img_w": int(img_w), "img_h": int(img_h), "boxes": boxes})
    return rows

def _fit_box_to_image(box, img_w, img_h):
    x = max(0, min(int(box["left"]), img_w - 1))
    y = max(0, min(int(box["top"]), img_h - 1))
    w = max(1, min(int(box["width"]), img_w - x))
    h = max(1, min(int(box["height"]), img_h - y))
    return x, y, w, h

def build_dataset_from_annotations(appcfg: AppConfig, dest_root: Path, val_ratio: float = 0.2, img_size: int = 224):
    rows = _load_annotations_csv()
    if not rows:
        raise RuntimeError("No annotations found. Create and save annotations first.")

    _ensure_dirs(dest_root)
    fetcher = ImageFetcher(appcfg)

    # Shuffle for split
    idxs = list(range(len(rows)))
    random.shuffle(idxs)
    val_cut = int(len(rows) * (1 - val_ratio))

    saved_counts = {"train": {"Genuine": 0, "Unknown Part": 0}, "val": {"Genuine": 0, "Unknown Part": 0}}

    for k, i in enumerate(idxs):
        row = rows[i]
        split = "train" if k < val_cut else "val"
        label = row["label"]
        url = row["url"]

        try:
            img = fetcher.fetch(url)  # original size
        except Exception as e:
            LOGGER.warning("Skip %s due to fetch error: %s", url, e)
            continue

        if label == "Genuine":
            # Use full image
            out_img = ImageOps.exif_transpose(img).convert("RGB")
            out_path = dest_root / split / "Genuine" / f"{sha256_of_text(url)}.jpg"
            out_img.save(out_path, format="JPEG", quality=92)
            saved_counts[split]["Genuine"] += 1
        elif label == "Unknown Part":
            boxes = row.get("boxes", [])
            if not boxes:
                # fallback to full image as unknown (worst case)
                out_img = ImageOps.exif_transpose(img).convert("RGB")
                out_path = dest_root / split / "Unknown Part" / f"{sha256_of_text(url)}.jpg"
                out_img.save(out_path, format="JPEG", quality=92)
                saved_counts[split]["Unknown Part"] += 1
            else:
                # Save each crop as one sample
                for j, b in enumerate(boxes):
                    x, y, w, h = _fit_box_to_image(b, img.width, img.height)
                    crop = img.crop((x, y, x + w, y + h)).convert("RGB")
                    out_path = dest_root / split / "Unknown Part" / f"{sha256_of_text(url)}_{j}.jpg"
                    crop.save(out_path, format="JPEG", quality=92)
                    saved_counts[split]["Unknown Part"] += 1
        else:
            LOGGER.warning("Unknown label in annotations: %s", label)

    LOGGER.info("Built dataset at %s -> %s", dest_root, saved_counts)
    return saved_counts

# =============================
# Streamlit UI
# =============================
def ui_header():
    st.title("📱 iPhone Unknown Part Detector")
    st.caption("Paste an image URL or upload a file. Train once, predict anytime. Logs, metrics, and annotation included.")

def ui_sidebar(tcfg: TrainConfig, appcfg: AppConfig) -> Tuple[TrainConfig, AppConfig]:
    st.sidebar.header("⚙️ Settings")
    with st.sidebar.expander("Training", expanded=False):
        model_name = st.selectbox("Model", ["resnet50", "resnet18", "efficientnet_v2_s"], index=["resnet50", "resnet18", "efficientnet_v2_s"].index(tcfg.model_name), key="sidebar_model_select")
        freeze = st.checkbox("Freeze backbone", value=tcfg.freeze_backbone, key="sidebar_freeze_checkbox")
        img_size = st.slider("Image size", 160, 384, tcfg.img_size, step=16, key="sidebar_img_size_slider")
        batch_size = st.slider("Batch size", 4, 64, tcfg.batch_size, step=4, key="sidebar_batch_size_slider")
        epochs = st.slider("Epochs", 1, 50, tcfg.epochs, step=1, key="sidebar_epochs_slider")
        lr = st.number_input("Learning rate", value=float(tcfg.lr), min_value=1e-6, max_value=1e-1, step=1e-5, format="%.6f", key="sidebar_lr_input")
        weight_decay = st.number_input("Weight decay", value=float(tcfg.weight_decay), min_value=0.0, max_value=1e-2, step=1e-5, format="%.6f", key="sidebar_weight_decay_input")
        early_stop = st.slider("Early stop patience", 1, 15, tcfg.early_stop_patience, key="sidebar_early_stop_slider")
        amp = st.checkbox("Mixed precision (AMP)", value=tcfg.amp, key="sidebar_amp_checkbox")
        mixup = st.number_input("Mixup alpha", value=float(tcfg.mixup_alpha), min_value=0.0, max_value=1.0, step=0.1, key="sidebar_mixup_input")
        cutmix = st.number_input("CutMix alpha", value=float(tcfg.cutmix_alpha), min_value=0.0, max_value=1.0, step=0.1, key="sidebar_cutmix_input")
        data_dir = st.text_input("Dataset path (for training/eval)", value=tcfg.data_dir, key="sidebar_data_dir_input")

    with st.sidebar.expander("App", expanded=False):
        timeout = st.slider("Request timeout (s)", 3, 60, appcfg.request_timeout, key="sidebar_timeout_slider")
        max_mb = st.slider("Max image size (MB)", 1, 50, appcfg.max_image_size_mb, key="sidebar_max_mb_slider")
        st.caption("Allowed content types: image/*")



    new_tcfg = TrainConfig(
        data_dir=data_dir,
        img_size=img_size,
        batch_size=batch_size,
        epochs=epochs,
        lr=lr,
        weight_decay=weight_decay,
        early_stop_patience=early_stop,
        model_name=model_name,
        freeze_backbone=freeze,
        mixup_alpha=mixup,
        cutmix_alpha=cutmix,
        amp=amp,
        num_workers=tcfg.num_workers,
    )
    # new_appcfg is not defined in this function, so fix return to use appcfg
    return new_tcfg, appcfg

def ui_predict_single(appcfg: AppConfig, tcfg: TrainConfig):
    st.subheader("🔎 Predict (Single Image)")
    url = st.text_input("Image URL", key="predict_single_url_input")
    uploaded = st.file_uploader("...or upload an image file", type=["jpg", "jpeg", "png", "webp"], key="predict_single_file_uploader")
    fetcher = ImageFetcher(appcfg)
    engine = None
    img: Optional[Image.Image] = None

    if st.button("Predict", use_container_width=True, key="predict_single_button"):
        try:
            if uploaded is not None:
                data = uploaded.read()
                img = Image.open(io.BytesIO(data)).convert("RGB")
            else:
                if not url:
                    st.warning("Please provide a URL or upload a file.")
                    return
                img = fetcher.fetch(url)

            engine = InferenceEngine(img_size=tcfg.img_size, model_name=tcfg.model_name)
            label, conf = engine.predict_pil(img)
            st.image(img, caption=f"Prediction: {label} ({conf*100:.2f}%)")
            st.success(f"Result: {label} with {conf*100:.2f}% confidence")
            LOGGER.info("Prediction -> %s (%.2f%%)", label, conf * 100)
        except Exception as e:
            LOGGER.exception("Prediction failed: %s", e)
            st.error(f"Prediction failed: {e}")

def ui_predict_batch(appcfg: AppConfig, tcfg: TrainConfig):
    st.subheader("📦 Batch Predict (CSV/Lines of URLs)")
    st.caption("Paste one URL per line, or comma-separated. We'll fetch each and predict.")
    text = st.text_area("URLs", height=160, key="predict_batch_text_area")
    if st.button("Run Batch", use_container_width=True, key="predict_batch_button"):
        urls = parse_csv_urls(text)
        if not urls:
            st.warning("No valid URLs found.")
            return
        fetcher = ImageFetcher(appcfg)
        engine = None
        results: List[Tuple[str, str, float]] = []  # (url, label, conf)
        progress = st.progress(0.0, text="Fetching...", key="predict_batch_progress")
        for i, u in enumerate(urls):
            try:
                img = fetcher.fetch(u)
                if engine is None:
                    engine = InferenceEngine(img_size=tcfg.img_size, model_name=tcfg.model_name)
                label, conf = engine.predict_pil(img)
                results.append((u, label, conf))
            except Exception as e:
                LOGGER.warning("Batch item failed for %s: %s", u, e)
                results.append((u, f"ERROR: {e}", 0.0))
            progress.progress((i + 1) / len(urls), text=f"Processed {i+1}/{len(urls)}")
        progress.empty()

        st.write("### Results")
        for u, label, conf in results:
            st.write(f"- {u} -> **{label}** ({conf*100:.2f}%)")

        csv_lines = ["url,label,confidence"] + [f"{u},{label},{conf:.4f}" for u, label, conf in results]
        csv_data = "\n".join(csv_lines).encode("utf-8")
        st.download_button("Download CSV", data=csv_data, file_name="batch_results.csv", mime="text/csv")

def ui_evaluate(tcfg: TrainConfig):
    st.subheader("🧪 Evaluate on Validation Set")
    data_dir = st.text_input("Dataset path", value=tcfg.data_dir, key="evaluate_data_dir_input")
    if st.button("Evaluate", use_container_width=True, key="evaluate_button"):
        try:
            set_torch_seed()
            val_dir = Path(data_dir) / "val"
            if not val_dir.exists():
                st.error("Validation folder not found.")
                return
            class_names = json.loads(LABELS_PATH.read_text()) if LABELS_PATH.exists() else datasets.ImageFolder(val_dir).classes
            ds = datasets.ImageFolder(val_dir, transform=build_transforms(tcfg.img_size, False))
            loader = DataLoader(ds, batch_size=tcfg.batch_size, shuffle=False, num_workers=tcfg.num_workers)
            engine = InferenceEngine(img_size=tcfg.img_size, model_name=tcfg.model_name)
            cm = [[0 for _ in class_names] for _ in class_names]
            correct = 0
            total = 0
            for inputs, targets in loader:
                inputs = inputs.to(engine.device)
                targets = targets.to(engine.device)
                with torch.inference_mode():
                    outputs = engine.model(inputs)
                    _, preds = torch.max(outputs, 1)
                for t, p in zip(targets.cpu().tolist(), preds.cpu().tolist()):
                    cm[t][p] += 1
                    if t == p:
                        correct += 1
                    total += 1
            acc = correct / max(1, total)
            LOGGER.info("Eval acc=%.3f on %d samples", acc, total)
            img_bytes = plot_confusion_matrix(cm, class_names)
            st.image(img_bytes, caption=f"Confusion Matrix (acc={acc*100:.2f}%)")
        except Exception as e:
            LOGGER.exception("Evaluation failed: %s", e)
            st.error(f"Evaluation failed: {e}")

def ui_metrics():
    st.subheader("📈 Metrics & Artifacts")
    cols = st.columns(3)
    with cols[0]:
        st.write("**Best model**", str(MODEL_PATH_BEST))
        st.write("**Last model**", str(MODEL_PATH_LAST))
        st.write("**TorchScript**", str(MODEL_TORCHSCRIPT))
    with cols[1]:
        st.write("**Class names**", str(LABELS_PATH))
        st.write("**History**", str(HISTORY_PATH))
        st.write("**Metrics**", str(METRICS_PATH))
    with cols[2]:
        st.write("**Logs**", str(LOG_FILE))
        st.write("**Annotations**", str(ANNOTATIONS_CSV))
        st.write("**Image cache**", str(IMAGE_CACHE_DIR))

    if HISTORY_PATH.exists():
        hist = json.loads(HISTORY_PATH.read_text())
        img_bytes = render_training_curves(hist)
        st.image(img_bytes, caption="Training Curves")

    if MODEL_PATH_BEST.exists():
        st.download_button("Download Best Model (.pth)", data=MODEL_PATH_BEST.read_bytes(), file_name="model_best.pth")
    if MODEL_TORCHSCRIPT.exists():
        st.download_button("Download TorchScript (.pt)", data=MODEL_TORCHSCRIPT.read_bytes(), file_name="model_scripted.pt")



# =============================
# STREAMLIT UI
# =============================
def ui_header():
    st.title("📱 iPhone Unknown Part Detector")
    st.caption("Paste an image URL or upload a file. Train once, predict anytime. Logs, metrics, and annotation included.")

def ui_sidebar(tcfg: TrainConfig, appcfg: AppConfig) -> Tuple[TrainConfig, AppConfig]:
    st.sidebar.header("⚙️ Settings")
    with st.sidebar.expander("Training", expanded=False):
        model_name = st.selectbox("Model", ["resnet50", "resnet18", "efficientnet_v2_s"], index=["resnet50", "resnet18", "efficientnet_v2_s"].index(tcfg.model_name))
        freeze = st.checkbox("Freeze backbone", value=tcfg.freeze_backbone)
        img_size = st.slider("Image size", 160, 384, tcfg.img_size, step=16)
        batch_size = st.slider("Batch size", 4, 64, tcfg.batch_size, step=4)
        epochs = st.slider("Epochs", 1, 50, tcfg.epochs, step=1)
        lr = st.number_input("Learning rate", value=float(tcfg.lr), min_value=1e-6, max_value=1e-1, step=1e-5, format="%.6f")
        weight_decay = st.number_input("Weight decay", value=float(tcfg.weight_decay), min_value=0.0, max_value=1e-2, step=1e-5, format="%.6f")
        early_stop = st.slider("Early stop patience", 1, 15, tcfg.early_stop_patience)
        amp = st.checkbox("Mixed precision (AMP)", value=tcfg.amp)
        mixup = st.number_input("Mixup alpha", value=float(tcfg.mixup_alpha), min_value=0.0, max_value=1.0, step=0.1)
        cutmix = st.number_input("CutMix alpha", value=float(tcfg.cutmix_alpha), min_value=0.0, max_value=1.0, step=0.1)
        data_dir = st.text_input("Dataset path (for training/eval)", value=tcfg.data_dir)

    with st.sidebar.expander("App", expanded=False):
        timeout = st.slider("Request timeout (s)", 3, 60, appcfg.request_timeout)
        max_mb = st.slider("Max image size (MB)", 1, 50, appcfg.max_image_size_mb)
        st.caption("Allowed content types: image/*")

    new_tcfg = TrainConfig(
        data_dir=data_dir,
        img_size=img_size,
        batch_size=batch_size,
        epochs=epochs,
        lr=lr,
        weight_decay=weight_decay,
        early_stop_patience=early_stop,
        model_name=model_name,
        freeze_backbone=freeze,
        mixup_alpha=mixup,
        cutmix_alpha=cutmix,
        amp=amp,
        num_workers=tcfg.num_workers,
    )
    new_appcfg = AppConfig(request_timeout=timeout, max_image_size_mb=max_mb,
                           allowed_content_types=appcfg.allowed_content_types,
                           retry_attempts=DEFAULT_APP_CONFIG.retry_attempts,
                           retry_backoff_sec=DEFAULT_APP_CONFIG.retry_backoff_sec)
    return new_tcfg, new_appcfg

def ui_logs():
    st.subheader("📜 Recent Logs")
    st.code("\n".join(STREAMLIT_HANDLER.buffer[-300:]) or "<no logs yet>")
    st.download_button("Download full log file", data=LOG_FILE.read_bytes() if LOG_FILE.exists() else b"",
                       file_name="app.log", mime="text/plain")

def ui_train(tcfg: TrainConfig):
    st.subheader("🧠 Train Model")
    data_dir = st.text_input("Dataset path (must contain 'train' and 'val' subfolders)", value=tcfg.data_dir)
    col1, col2 = st.columns(2)
    start = col1.button("Start Training", use_container_width=True)
    abort = col2.button("Abort Training", use_container_width=True)

    if start:
        try:
            st.session_state["ui_mode"] = True
            set_torch_seed()
            trainer = Trainer(tcfg, data_dir)
            save_runtime_config(tcfg, DEFAULT_APP_CONFIG)
            trainer.train()
            st.success("Training completed. Best model saved.")
            if HISTORY_PATH.exists():
                hist = json.loads(HISTORY_PATH.read_text())
                img_bytes = render_training_curves(hist)
                st.image(img_bytes)
        except Exception as e:
            LOGGER.exception("Training failed: %s", e)
            st.error(f"Training failed: {e}")

    if abort:
        try:
            os.kill(os.getpid(), signal.SIGINT)
        except Exception:
            pass

def ui_predict_single(appcfg: AppConfig, tcfg: TrainConfig):
    st.subheader("🔎 Predict (Single Image)")
    url = st.text_input("Image URL")
    uploaded = st.file_uploader("...or upload an image file", type=["jpg", "jpeg", "png", "webp"])
    fetcher = ImageFetcher(appcfg)
    engine = None
    img: Optional[Image.Image] = None

    if st.button("Predict", use_container_width=True):
        try:
            if uploaded is not None:
                data = uploaded.read()
                img = Image.open(io.BytesIO(data)).convert("RGB")
            else:
                if not url:
                    st.warning("Please provide a URL or upload a file.")
                    return
                img = fetcher.fetch(url)

            engine = InferenceEngine(img_size=tcfg.img_size, model_name=tcfg.model_name)
            label, conf = engine.predict_pil(img)
            st.image(img, caption=f"Prediction: {label} ({conf*100:.2f}%)")
            st.success(f"Result: {label} with {conf*100:.2f}% confidence")
            LOGGER.info("Prediction -> %s (%.2f%%)", label, conf * 100)
        except Exception as e:
            LOGGER.exception("Prediction failed: %s", e)
            st.error(f"Prediction failed: {e}")

def ui_predict_batch(appcfg: AppConfig, tcfg: TrainConfig):
    st.subheader("📦 Batch Predict (CSV/Lines of URLs)")
    st.caption("Paste one URL per line, or comma-separated. We'll fetch each and predict.")
    text = st.text_area("URLs", height=160)
    if st.button("Run Batch", use_container_width=True):
        urls = parse_csv_urls(text)
        if not urls:
            st.warning("No valid URLs found.")
            return
        fetcher = ImageFetcher(appcfg)
        engine = None
        results: List[Tuple[str, str, float]] = []  # (url, label, conf)
        progress = st.progress(0.0, text="Fetching...")
        for i, u in enumerate(urls):
            try:
                img = fetcher.fetch(u)
                if engine is None:
                    engine = InferenceEngine(img_size=tcfg.img_size, model_name=tcfg.model_name)
                label, conf = engine.predict_pil(img)
                results.append((u, label, conf))
            except Exception as e:
                LOGGER.warning("Batch item failed for %s: %s", u, e)
                results.append((u, f"ERROR: {e}", 0.0))
            progress.progress((i + 1) / len(urls), text=f"Processed {i+1}/{len(urls)}")
        progress.empty()

        st.write("### Results")
        for u, label, conf in results:
            st.write(f"- {u} -> **{label}** ({conf*100:.2f}%)")

        csv_lines = ["url,label,confidence"] + [f"{u},{label},{conf:.4f}" for u, label, conf in results]
        csv_data = "\n".join(csv_lines).encode("utf-8")
        st.download_button("Download CSV", data=csv_data, file_name="batch_results.csv", mime="text/csv")

def ui_evaluate(tcfg: TrainConfig):
    st.subheader("🧪 Evaluate on Validation Set")
    data_dir = st.text_input("Dataset path", value=tcfg.data_dir)
    if st.button("Evaluate", use_container_width=True):
        try:
            set_torch_seed()
            val_dir = Path(data_dir) / "val"
            if not val_dir.exists():
                st.error("Validation folder not found.")
                return
            class_names = json.loads(LABELS_PATH.read_text()) if LABELS_PATH.exists() else datasets.ImageFolder(val_dir).classes
            ds = datasets.ImageFolder(val_dir, transform=build_transforms(tcfg.img_size, False))
            loader = DataLoader(ds, batch_size=tcfg.batch_size, shuffle=False, num_workers=tcfg.num_workers)
            engine = InferenceEngine(img_size=tcfg.img_size, model_name=tcfg.model_name)
            cm = [[0 for _ in class_names] for _ in class_names]
            correct = 0
            total = 0
            for inputs, targets in loader:
                inputs = inputs.to(engine.device)
                targets = targets.to(engine.device)
                with torch.inference_mode():
                    outputs = engine.model(inputs)
                    _, preds = torch.max(outputs, 1)
                for t, p in zip(targets.cpu().tolist(), preds.cpu().tolist()):
                    cm[t][p] += 1
                    if t == p:
                        correct += 1
                    total += 1
            acc = correct / max(1, total)
            LOGGER.info("Eval acc=%.3f on %d samples", acc, total)
            img_bytes = plot_confusion_matrix(cm, class_names)
            st.image(img_bytes, caption=f"Confusion Matrix (acc={acc*100:.2f}%)")
        except Exception as e:
            LOGGER.exception("Evaluation failed: %s", e)
            st.error(f"Evaluation failed: {e}")

def ui_metrics():
    st.subheader("📈 Metrics & Artifacts")
    cols = st.columns(3)
    with cols[0]:
        st.write("**Best model**", str(MODEL_PATH_BEST))
        st.write("**Last model**", str(MODEL_PATH_LAST))
        st.write("**TorchScript**", str(MODEL_TORCHSCRIPT))
    with cols[1]:
        st.write("**Class names**", str(LABELS_PATH))
        st.write("**History**", str(HISTORY_PATH))
        st.write("**Metrics**", str(METRICS_PATH))
    with cols[2]:
        st.write("**Logs**", str(LOG_FILE))
        st.write("**Annotations**", str(ANNOTATIONS_CSV))
        st.write("**Image cache**", str(IMAGE_CACHE_DIR))

    if HISTORY_PATH.exists():
        hist = json.loads(HISTORY_PATH.read_text())
        img_bytes = render_training_curves(hist)
        st.image(img_bytes, caption="Training Curves")

    if MODEL_PATH_BEST.exists():
        st.download_button("Download Best Model (.pth)", data=MODEL_PATH_BEST.read_bytes(), file_name="model_best.pth")
    if MODEL_TORCHSCRIPT.exists():
        st.download_button("Download TorchScript (.pt)", data=MODEL_TORCHSCRIPT.read_bytes(), file_name="model_scripted.pt")



def ui_annotate(appcfg: AppConfig, tcfg: TrainConfig):

    pass
    st.subheader("✏️ Annotate Images (Bounding Boxes for 'Unknown Part')")
    st.caption("Paste URLs, draw boxes, save to CSV, and build a dataset for training.")
    url_text = st.text_area("Paste image URLs (one per line)", height=120, key="annotate_url_text_area")
    urls = [u.strip() for u in url_text.splitlines() if u.strip()]
    if not urls:
        st.info("Paste one or more image URLs above.")
        return
    idx = st.number_input("Image index", min_value=0, max_value=max(0, len(urls)-1), value=0, step=1, key="annotate_idx_number_input")
    url = urls[idx]
    fetcher = ImageFetcher(appcfg)
    try:
        img = fetcher.fetch(url)
    except Exception as e:
        st.error(f"Failed to fetch image: {e}")
        return
    st.image(img, caption=f"Image {idx+1}/{len(urls)}: {url}")
    label = st.selectbox("Label", ["Genuine", "Unknown Part"], index=1, key=f"label_select_{idx}")
    st.caption("Draw bounding boxes for 'Unknown Part'. For 'Genuine', no boxes needed.")
    boxes = []
    if label == "Unknown Part":
        canvas_result = st_canvas(
            fill_color="rgba(255, 0, 0, 0.3)",
            stroke_width=3,
            stroke_color="#ff0000",
            background_image=img,
            update_streamlit=True,
            height=img.height,
            width=img.width,
            drawing_mode="rect",
            key=f"canvas_{idx}"
        )
        if canvas_result.json_data is not None:
            for obj in canvas_result.json_data.get("objects", []):
                left = obj.get("left", 0)
                top = obj.get("top", 0)
                width = obj.get("width", 0)
                height = obj.get("height", 0)
                boxes.append({"left": left, "top": top, "width": width, "height": height})
    # Save annotation
    if st.button("Save Annotation", use_container_width=True, key=f"annotate_save_button_{idx}"):
        if 'rows' in st.session_state:
            rows = st.session_state['rows']
        else:
            rows = []
        try:
            from pathlib import Path
            # Try to load existing annotations if function exists
            if '_load_annotations_csv' in globals():
                rows = _load_annotations_csv()
        except Exception:
            pass
        # Remove any existing annotation for this url
        rows = [r for r in rows if r.get("url") != url]
        rows.append({"url": url, "label": label, "img_w": img.width, "img_h": img.height, "boxes": boxes})
        if '_save_annotations_csv' in globals():
            _save_annotations_csv(rows)
        st.session_state['rows'] = rows
        st.success("Annotation saved.")
    # Build dataset
    if st.button("Build Dataset from Annotations", use_container_width=True, key=f"annotate_build_button_{idx}"):
        try:
            dest_root = Path(tcfg.data_dir)
            if 'build_dataset_from_annotations' in globals():
                result = build_dataset_from_annotations(appcfg, dest_root, val_ratio=0.2, img_size=tcfg.img_size)
                st.success(f"Dataset built: {result}")
            else:
                st.warning("build_dataset_from_annotations function not found.")
        except Exception as e:
            st.error(f"Failed to build dataset: {e}")
    st.session_state["ui_mode"] = True
    ui_header()

def main():
    tcfg, appcfg = DEFAULT_TRAIN_CONFIG, DEFAULT_APP_CONFIG
    tcfg, appcfg = ui_sidebar(tcfg, appcfg)
    tabs = st.tabs(["Predict", "Batch", "Annotate", "Train", "Evaluate", "Metrics", "Logs"])
    with tabs[0]:
        ui_predict_single(appcfg, tcfg)
    with tabs[1]:
        ui_predict_batch(appcfg, tcfg)
    with tabs[2]:
        ui_annotate(appcfg, tcfg)
    with tabs[3]:
        ui_train(tcfg)
    with tabs[4]:
        ui_evaluate(tcfg)
    with tabs[5]:
        ui_metrics()
    with tabs[6]:
        ui_logs()

if __name__ == "__main__":
    main()